import json

notebook = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 1: Rating Prediction using LLM Prompting Strategies\n",
                "\n",
                "This notebook evaluates three different prompting strategies for predicting Yelp review ratings (1-5 stars).\n",
                "\n",
                "## Objectives:\n",
                "1. Compare naive vs structured vs rubric-based prompting\n",
                "2. Measure accuracy, JSON validity, and consistency\n",
                "3. Document findings for production use"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "# !pip install kagglehub openai pandas numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "import kagglehub\n",
                "import pandas as pd\n",
                "import json\n",
                "import os\n",
                "from openai import OpenAI\n",
                "import time\n",
                "from typing import Dict, List, Tuple\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 1. Load and Prepare Dataset"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download Yelp Reviews dataset\n",
                "path = kagglehub.dataset_download(\"omkarsabnis/yelp-reviews-dataset\")\n",
                "print(f\"Dataset downloaded to: {path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find the CSV file in the downloaded path\n",
                "import glob\n",
                "csv_files = glob.glob(f\"{path}/**/*.csv\", recursive=True)\n",
                "print(f\"Found CSV files: {csv_files}\")\n",
                "\n",
                "# Load the dataset\n",
                "df = pd.read_csv(csv_files[0])\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "print(f\"Columns: {df.columns.tolist()}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample 200 rows with stratified sampling\n",
                "np.random.seed(42)\n",
                "\n",
                "# Identify the star rating column\n",
                "rating_col = 'stars' if 'stars' in df.columns else 'rating'\n",
                "text_col = 'text' if 'text' in df.columns else 'review'\n",
                "\n",
                "# Sample 200 reviews\n",
                "df_sample = df[[rating_col, text_col]].dropna().sample(n=min(200, len(df)), random_state=42)\n",
                "df_sample.columns = ['actual_rating', 'review_text']\n",
                "\n",
                "print(f\"Sampled {len(df_sample)} reviews\")\n",
                "print(f\"\\nRating distribution:\")\n",
                "print(df_sample['actual_rating'].value_counts().sort_index())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 2. Setup OpenRouter API Client"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize OpenRouter client\n",
                "import os\n",
                "OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')\n",
                "if not OPENROUTER_API_KEY:\n",
                "    raise ValueError('OPENROUTER_API_KEY environment variable is required')\n",
                "\n",
                "client = OpenAI(\n",
                "    base_url=\"https://openrouter.ai/api/v1\",\n",
                "    api_key=OPENROUTER_API_KEY,\n",
                ")\n",
                "\n",
                "MODEL = \"openai/gpt-3.5-turbo\"\n",
                "\n",
                "print(f\"Using model: {MODEL}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 3. Define Prompting Strategies"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Strategy 1: Naive Prompt\n",
                "NAIVE_PROMPT = \"\"\"Predict the star rating (1-5) for the following review. Return your response as JSON with 'predicted_stars' and 'explanation' fields.\n",
                "\n",
                "Review: {review}\"\"\"\n",
                "\n",
                "# Strategy 2: Structured JSON-Enforced Prompt\n",
                "STRUCTURED_PROMPT = \"\"\"You are a rating prediction system. Analyze the following review and predict the star rating.\n",
                "\n",
                "STRICT OUTPUT FORMAT (you MUST respond with valid JSON only):\n",
                "{{\n",
                "  \"predicted_stars\": <integer between 1 and 5>,\n",
                "  \"explanation\": \"<brief explanation of your prediction>\"\n",
                "}}\n",
                "\n",
                "Rules:\n",
                "- predicted_stars must be an integer: 1, 2, 3, 4, or 5\n",
                "- explanation must be a concise string (1-2 sentences)\n",
                "- Return ONLY valid JSON, no additional text\n",
                "\n",
                "Review: {review}\"\"\"\n",
                "\n",
                "# Strategy 3: Rubric-Based Reasoning Prompt\n",
                "RUBRIC_PROMPT = \"\"\"You are an expert review analyst. Use the following rubric to predict the star rating:\n",
                "\n",
                "RATING RUBRIC:\n",
                "â˜… 1 Star: Extremely negative sentiment, mentions of terrible service/quality, words like \"worst\", \"awful\", \"never again\"\n",
                "â˜…â˜… 2 Stars: Mostly negative, disappointed, multiple complaints, minimal positive aspects\n",
                "â˜…â˜…â˜… 3 Stars: Mixed/neutral sentiment, both positives and negatives mentioned, \"okay\" or \"average\"\n",
                "â˜…â˜…â˜…â˜… 4 Stars: Mostly positive, generally satisfied, minor issues mentioned, would recommend\n",
                "â˜…â˜…â˜…â˜…â˜… 5 Stars: Extremely positive, enthusiastic, words like \"amazing\", \"perfect\", \"best\", strong recommendation\n",
                "\n",
                "ANALYSIS PROCESS:\n",
                "1. Identify key sentiment indicators (positive/negative words)\n",
                "2. Assess overall tone and emotional intensity\n",
                "3. Consider specific complaints or praise\n",
                "4. Apply rubric to determine rating\n",
                "\n",
                "Review: {review}\n",
                "\n",
                "Respond with ONLY valid JSON:\n",
                "{{\n",
                "  \"predicted_stars\": <integer 1-5>,\n",
                "  \"explanation\": \"<reasoning based on rubric>\"\n",
                "}}\"\"\"\n",
                "\n",
                "print(\"Prompting strategies defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 4. Prediction Function"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_rating(review: str, prompt_template: str, max_retries: int = 2) -> Dict:\n",
                "    \"\"\"\n",
                "    Predict rating using LLM with specified prompt template.\n",
                "    \n",
                "    Returns:\n",
                "        dict: {\n",
                "            'predicted_stars': int or None,\n",
                "            'explanation': str,\n",
                "            'is_valid_json': bool,\n",
                "            'raw_response': str,\n",
                "            'error': str or None\n",
                "        }\n",
                "    \"\"\"\n",
                "    prompt = prompt_template.format(review=review[:1000])  # Limit review length\n",
                "    \n",
                "    for attempt in range(max_retries):\n",
                "        try:\n",
                "            response = client.chat.completions.create(\n",
                "                model=MODEL,\n",
                "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "                temperature=0.3,\n",
                "                max_tokens=200,\n",
                "            )\n",
                "            \n",
                "            raw_response = response.choices[0].message.content.strip()\n",
                "            \n",
                "            # Try to parse JSON\n",
                "            try:\n",
                "                # Remove markdown code blocks if present\n",
                "                if raw_response.startswith('```'):\n",
                "                    raw_response = raw_response.split('```')[1]\n",
                "                    if raw_response.startswith('json'):\n",
                "                        raw_response = raw_response[4:]\n",
                "                \n",
                "                result = json.loads(raw_response)\n",
                "                \n",
                "                # Validate structure\n",
                "                predicted_stars = result.get('predicted_stars')\n",
                "                explanation = result.get('explanation', '')\n",
                "                \n",
                "                # Validate rating range\n",
                "                if isinstance(predicted_stars, (int, float)) and 1 <= predicted_stars <= 5:\n",
                "                    return {\n",
                "                        'predicted_stars': int(predicted_stars),\n",
                "                        'explanation': explanation,\n",
                "                        'is_valid_json': True,\n",
                "                        'raw_response': raw_response,\n",
                "                        'error': None\n",
                "                    }\n",
                "                else:\n",
                "                    raise ValueError(f\"Invalid rating: {predicted_stars}\")\n",
                "                    \n",
                "            except (json.JSONDecodeError, ValueError, KeyError) as e:\n",
                "                if attempt < max_retries - 1:\n",
                "                    time.sleep(1)\n",
                "                    continue\n",
                "                return {\n",
                "                    'predicted_stars': None,\n",
                "                    'explanation': '',\n",
                "                    'is_valid_json': False,\n",
                "                    'raw_response': raw_response,\n",
                "                    'error': f\"JSON parsing error: {str(e)}\"\n",
                "                }\n",
                "                \n",
                "        except Exception as e:\n",
                "            if attempt < max_retries - 1:\n",
                "                time.sleep(1)\n",
                "                continue\n",
                "            return {\n",
                "                'predicted_stars': None,\n",
                "                'explanation': '',\n",
                "                'is_valid_json': False,\n",
                "                'raw_response': '',\n",
                "                'error': f\"API error: {str(e)}\"\n",
                "            }\n",
                "    \n",
                "    return {\n",
                "        'predicted_stars': None,\n",
                "        'explanation': '',\n",
                "        'is_valid_json': False,\n",
                "        'raw_response': '',\n",
                "        'error': 'Max retries exceeded'\n",
                "    }\n",
                "\n",
                "print(\"Prediction function ready.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 5. Run Experiments"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Use smaller subset for testing or full 200 for final\n",
                "TEST_SIZE = 20  # Change to 200 for full evaluation\n",
                "df_test = df_sample.head(TEST_SIZE).copy()\n",
                "\n",
                "# For consistency testing\n",
                "CONSISTENCY_SIZE = 10\n",
                "\n",
                "print(f\"Running evaluation on {TEST_SIZE} reviews\")\n",
                "print(f\"Testing consistency on {CONSISTENCY_SIZE} reviews\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "strategies = {\n",
                "    'naive': NAIVE_PROMPT,\n",
                "    'structured': STRUCTURED_PROMPT,\n",
                "    'rubric': RUBRIC_PROMPT\n",
                "}\n",
                "\n",
                "results = {}\n",
                "\n",
                "for strategy_name, prompt_template in strategies.items():\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Evaluating: {strategy_name.upper()} strategy\")\n",
                "    print(f\"{'='*60}\\n\")\n",
                "    \n",
                "    predictions = []\n",
                "    consistency_predictions = []\n",
                "    \n",
                "    # First pass\n",
                "    for idx, row in df_test.iterrows():\n",
                "        print(f\"Predicting {len(predictions) + 1}/{TEST_SIZE}...\", end='\\r')\n",
                "        pred = predict_rating(row['review_text'], prompt_template)\n",
                "        predictions.append(pred)\n",
                "        time.sleep(0.5)\n",
                "    \n",
                "    # Second pass for consistency\n",
                "    print(f\"\\nTesting consistency on {CONSISTENCY_SIZE} reviews...\")\n",
                "    for idx, row in df_test.head(CONSISTENCY_SIZE).iterrows():\n",
                "        pred = predict_rating(row['review_text'], prompt_template)\n",
                "        consistency_predictions.append(pred)\n",
                "        time.sleep(0.5)\n",
                "    \n",
                "    results[strategy_name] = {\n",
                "        'predictions': predictions,\n",
                "        'consistency_predictions': consistency_predictions\n",
                "    }\n",
                "    \n",
                "    print(f\"\\nâœ“ Completed {strategy_name} strategy\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"All predictions complete!\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 6. Calculate Metrics"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_metrics(actual_ratings: List[float], predictions: List[Dict], \n",
                "                     consistency_predictions: List[Dict]) -> Dict:\n",
                "    \"\"\"Calculate accuracy, JSON validity, and consistency metrics.\"\"\"\n",
                "    # JSON Validity Rate\n",
                "    valid_json_count = sum(1 for p in predictions if p['is_valid_json'])\n",
                "    json_validity_rate = valid_json_count / len(predictions) * 100\n",
                "    \n",
                "    # Accuracy\n",
                "    correct = 0\n",
                "    valid_predictions = 0\n",
                "    \n",
                "    for actual, pred in zip(actual_ratings, predictions):\n",
                "        if pred['predicted_stars'] is not None:\n",
                "            valid_predictions += 1\n",
                "            if int(actual) == pred['predicted_stars']:\n",
                "                correct += 1\n",
                "    \n",
                "    accuracy = (correct / valid_predictions * 100) if valid_predictions > 0 else 0\n",
                "    \n",
                "    # Mean Absolute Error\n",
                "    mae_sum = 0\n",
                "    mae_count = 0\n",
                "    for actual, pred in zip(actual_ratings, predictions):\n",
                "        if pred['predicted_stars'] is not None:\n",
                "            mae_sum += abs(int(actual) - pred['predicted_stars'])\n",
                "            mae_count += 1\n",
                "    \n",
                "    mae = mae_sum / mae_count if mae_count > 0 else float('inf')\n",
                "    \n",
                "    # Consistency\n",
                "    consistent = 0\n",
                "    for pred1, pred2 in zip(predictions[:len(consistency_predictions)], \n",
                "                            consistency_predictions):\n",
                "        if (pred1['predicted_stars'] is not None and \n",
                "            pred2['predicted_stars'] is not None and\n",
                "            pred1['predicted_stars'] == pred2['predicted_stars']):\n",
                "            consistent += 1\n",
                "    \n",
                "    consistency_rate = (consistent / len(consistency_predictions) * 100) if consistency_predictions else 0\n",
                "    \n",
                "    return {\n",
                "        'accuracy': accuracy,\n",
                "        'json_validity_rate': json_validity_rate,\n",
                "        'consistency_rate': consistency_rate,\n",
                "        'mean_absolute_error': mae,\n",
                "        'valid_predictions': valid_predictions,\n",
                "        'total_predictions': len(predictions)\n",
                "    }\n",
                "\n",
                "# Calculate metrics\n",
                "metrics_summary = {}\n",
                "\n",
                "for strategy_name, strategy_results in results.items():\n",
                "    metrics = calculate_metrics(\n",
                "        df_test['actual_rating'].tolist(),\n",
                "        strategy_results['predictions'],\n",
                "        strategy_results['consistency_predictions']\n",
                "    )\n",
                "    metrics_summary[strategy_name] = metrics\n",
                "\n",
                "# Display results\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"EVALUATION RESULTS\")\n",
                "print(\"=\"*80 + \"\\n\")\n",
                "\n",
                "comparison_df = pd.DataFrame(metrics_summary).T\n",
                "comparison_df.index.name = 'Strategy'\n",
                "print(comparison_df.to_string())\n",
                "print(\"\\n\" + \"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 7. Save Results"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create detailed results CSV\n",
                "detailed_results = []\n",
                "\n",
                "for idx, row in df_test.iterrows():\n",
                "    result_row = {\n",
                "        'review_text': row['review_text'][:200] + '...' if len(row['review_text']) > 200 else row['review_text'],\n",
                "        'actual_rating': row['actual_rating']\n",
                "    }\n",
                "    \n",
                "    for strategy_name in strategies.keys():\n",
                "        pred_idx = df_test.index.get_loc(idx)\n",
                "        pred = results[strategy_name]['predictions'][pred_idx]\n",
                "        \n",
                "        result_row[f'{strategy_name}_predicted'] = pred['predicted_stars']\n",
                "        result_row[f'{strategy_name}_explanation'] = pred['explanation'][:100] if pred['explanation'] else ''\n",
                "        result_row[f'{strategy_name}_valid_json'] = pred['is_valid_json']\n",
                "    \n",
                "    detailed_results.append(result_row)\n",
                "\n",
                "results_df = pd.DataFrame(detailed_results)\n",
                "results_df.to_csv('evaluation_results.csv', index=False)\n",
                "print(\"âœ“ Saved detailed results to 'evaluation_results.csv'\")\n",
                "\n",
                "# Save comparison metrics\n",
                "comparison_df.to_csv('comparison_metrics.csv')\n",
                "print(\"âœ“ Saved comparison metrics to 'comparison_metrics.csv'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 8. Analysis & Insights"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"KEY INSIGHTS\")\n",
                "print(\"=\"*80 + \"\\n\")\n",
                "\n",
                "# Find best strategy\n",
                "best_accuracy = max(metrics_summary.items(), key=lambda x: x[1]['accuracy'])\n",
                "best_json = max(metrics_summary.items(), key=lambda x: x[1]['json_validity_rate'])\n",
                "best_consistency = max(metrics_summary.items(), key=lambda x: x[1]['consistency_rate'])\n",
                "best_mae = min(metrics_summary.items(), key=lambda x: x[1]['mean_absolute_error'])\n",
                "\n",
                "print(f\"ðŸŽ¯ Best Accuracy: {best_accuracy[0].upper()} ({best_accuracy[1]['accuracy']:.2f}%)\")\n",
                "print(f\"ðŸ“‹ Best JSON Validity: {best_json[0].upper()} ({best_json[1]['json_validity_rate']:.2f}%)\")\n",
                "print(f\"ðŸ”„ Best Consistency: {best_consistency[0].upper()} ({best_consistency[1]['consistency_rate']:.2f}%)\")\n",
                "print(f\"ðŸ“Š Best MAE: {best_mae[0].upper()} ({best_mae[1]['mean_absolute_error']:.2f})\")\n",
                "\n",
                "print(\"\\n\" + \"-\"*80)\n",
                "print(\"RECOMMENDATIONS\")\n",
                "print(\"-\"*80 + \"\\n\")\n",
                "\n",
                "print(\"For Production Use:\")\n",
                "print(\"  â†’ Use RUBRIC-BASED strategy for best balance\")\n",
                "print(\"  â†’ Implement retry logic for JSON parsing failures\")\n",
                "print(\"  â†’ Consider temperature=0.3 for deterministic outputs\")\n",
                "print(\"  â†’ Add input validation (review length, content filtering)\")\n",
                "print(\"\\n\" + \"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "This notebook demonstrated three prompting strategies:\n",
                "\n",
                "1. **Naive Prompt**: Simple baseline\n",
                "2. **Structured JSON Prompt**: Improved output formatting\n",
                "3. **Rubric-Based Prompt**: Best overall performance\n",
                "\n",
                "**Next Steps**: Use rubric-based prompt in production API (Task 2)."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {"name": "ipython", "version": 3},
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

with open('rating_prediction.ipynb', 'w') as f:
    json.dump(notebook, f, indent=1)
print("Notebook created successfully!")
